General plan for our model:

1. First we want to train a model that can predict high resolution audio, given a low resolution input.
The model will need to train with pairs of low and high resolution mel spectograms.
The low mel spectogram will be generated by performing a lowpass filter on the original waveform,
thus, creating a lower resolution waveform.

2. In the second stage, we would take the high resolution audio and add to it a single tone noise
(A sin wave with one frequency between let say 100Hz to 15kHz).
We will train our model again, now with pairs of original audio and audio with single tone noise,
in order to predict in the end the waveform without the single tone noise.

3. In the third stage, we would like to perform inpainting. The model will be trained with the original audio and with the same waveform but part of the waveform will be deleted (set to 0) in order to create a "damaged" audio file. We will set to 0 250ms of the waveform.
We would train the model with the pair of original waveform and damaged waveform, and we hope
that the model will be able to predict the missing sounds in time.



DATASET:
We will take the dataset from: MUSDB18-HQ or MedleyDB.
Only (5?) seconds from each sound track will be taken as an input.


Evaluation of results:


See this site for ideas on metrics for audio evaluation:
 https://medium.com/@poudelnipriyanka/audio-metrics-their-importance-and-their-necessity-417950b0d848

A. Objective measures: 
We will listen to the outputs and evaluate ourselves the quality of the predicted output.

B. Subjective measures:

1. SDR(Signal to Distortion Ratio)
SDR = 10 * log10(Es / Ed)

Where:

Es is the energy of the original or reference signal.
Ed is the energy of the distortion or noise in the degraded signal.

2. MSE

3. Spectral Distortion: This measures the difference between the spectral envelopes of two signals. Itâ€™s often calculated as the mean squared error between the LPC coefficients of the two signals


___________________________________________________________________________________________________
Plan A

1. take cog.yaml from AudioSR
2. add installation steps for audioldm-training-finetuning setup
3. remove packages that are only needed for cog deployment 
4. build image from it


Plan B

1. take cog.yaml from AudioSR and build image from it
2. export dockerfile of that image via cog debug
3. add installation steps for audioldm-training-finetuning setup
4. build another image from it

5. add steps for downloading datasets and checkpoints from google drive

7. Try to train a smaller model 
8. Make it work on windows



aux:

acquire compute resources from afeka, requirements:
-persistent 300 Gb storage
-GPU GeForce RTX 3090 with 24Gb vram
-ubuntu
-docker
-full access to internet
-ssh access preferably not with VMware horizon


done: 
disable docker service that runs as root
5. create docker run script for starting the container
6. in the script, map the dataset, checkpoints and ~/.cache folders to volumes outside of container


